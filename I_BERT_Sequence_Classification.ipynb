{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b97d18c",
   "metadata": {},
   "source": [
    "##  BERT demo as Masked Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fc4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a1c04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fd4b9262824fa79725574f1b8ae37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gorav sen\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gorav sen\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9703c77f16ff4c65b1528653650297eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41103a0aee8041c0ad521e3854c2a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb2d9c73a54413c9324771298142700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d848d7d7d6514a60bf98b18bf8362f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 51.35%\n",
      "Token:stop. Score: 39.66%\n",
      "Token:glance. Score: 1.02%\n",
      "Token:wait. Score: 0.60%\n",
      "Token:turn. Score: 0.57%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='bert-base-cased')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1c993940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token: look. Score: 47.69%\n",
      "Token: stop. Score: 36.82%\n",
      "Token: stand. Score: 2.54%\n",
      "Token: stay. Score: 2.52%\n",
      "Token: wave. Score: 1.01%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='roberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "81cceeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token: stop. Score: 42.11%\n",
      "Token: look. Score: 7.53%\n",
      "Token: park. Score: 4.92%\n",
      "Token: arrive. Score: 4.65%\n",
      "Token: sign. Score: 4.27%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilroberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6dbf2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 57.47%\n",
      "Token:stop. Score: 7.37%\n",
      "Token:glance. Score: 3.74%\n",
      "Token:arrive. Score: 2.16%\n",
      "Token:appear. Score: 1.87%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilbert-base-cased')  # Using a flavor of BERT called DistilBERT\n",
    "\n",
    "print(type(nlp.model))  \n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098dcb1",
   "metadata": {},
   "source": [
    "## BERT for sequence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2484167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, \\\n",
    "     DataCollatorWithPadding, pipeline\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4e17957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'listen O\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'westbam B-artist\\r\\n',\n",
       " b'alumb O\\r\\n',\n",
       " b'allergic B-album\\r\\n',\n",
       " b'on O\\r\\n',\n",
       " b'google B-service\\r\\n',\n",
       " b'music I-service\\r\\n',\n",
       " b'PlayMusic\\r\\n',\n",
       " b'\\r\\n',\n",
       " b'add O\\r\\n',\n",
       " b'step B-entity_name\\r\\n',\n",
       " b'to I-entity_name\\r\\n',\n",
       " b'me I-entity_name\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'the O\\r\\n',\n",
       " b'50 B-playlist\\r\\n',\n",
       " b'cl\\xc3\\xa1sicos I-playlist\\r\\n',\n",
       " b'playlist O\\r\\n',\n",
       " b'AddToPlaylist\\r\\n']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_file = open('../data/snips.train.txt', 'rb')\n",
    "\n",
    "snips_rows = snips_file.readlines()\n",
    "\n",
    "snips_rows[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9f7b8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the snips dataset into a more manageable format\n",
    "\n",
    "utterances = []\n",
    "tokenized_utterances = []\n",
    "labels_for_tokens = []\n",
    "sequence_labels = []\n",
    "\n",
    "utterance, tokenized_utterance, label_for_utterances = '', [], []\n",
    "for snip_row in snips_rows:\n",
    "    if len(snip_row) == 2:  # skip over rows with no data\n",
    "        continue\n",
    "    if ' ' not in snip_row.decode():  # we've hit a sequence label\n",
    "        sequence_labels.append(snip_row.decode().strip())\n",
    "        utterances.append(utterance.strip())\n",
    "        tokenized_utterances.append(tokenized_utterance)\n",
    "        labels_for_tokens.append(label_for_utterances)\n",
    "        utterance = ''\n",
    "        tokenized_utterance = []\n",
    "        label_for_utterances = []\n",
    "        continue\n",
    "    token, token_label = snip_row.decode().split(' ')\n",
    "    token_label = token_label.strip()\n",
    "    utterance += f'{token} '\n",
    "    tokenized_utterance.append(token)\n",
    "    label_for_utterances.append(token_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c78793ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13084, 13084, 13084, 13084)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_for_tokens), len(tokenized_utterances), len(utterances), len(sequence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "774c7c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bb5d631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PlayMusic',\n",
       " 'RateBook',\n",
       " 'SearchScreeningEvent',\n",
       " 'SearchCreativeWork',\n",
       " 'BookRestaurant',\n",
       " 'GetWeather',\n",
       " 'AddToPlaylist']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3e9fce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique sequence labels\n"
     ]
    }
   ],
   "source": [
    "sequence_labels = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "\n",
    "print(f'There are {len(unique_sequence_labels)} unique sequence labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6de12c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 unique token labels\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "unique_token_labels = list(set(reduce(lambda x, y: x + y, labels_for_tokens)))\n",
    "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
    "\n",
    "print(f'There are {len(unique_token_labels)} unique token labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3a5251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'to', 'westbam', 'alumb', 'allergic', 'on', 'google', 'music']\n",
      "[22, 22, 68, 22, 58, 22, 51, 4]\n",
      "['O', 'O', 'B-artist', 'O', 'B-album', 'O', 'B-service', 'I-service']\n",
      "listen to westbam alumb allergic on google music\n",
      "0\n",
      "PlayMusic\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_utterances[0])\n",
    "print(labels_for_tokens[0])\n",
    "print([unique_token_labels[l] for l in labels_for_tokens[0]])\n",
    "print(utterances[0])\n",
    "print(sequence_labels[0])\n",
    "print(unique_sequence_labels[sequence_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "eec863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        utterance=utterances, \n",
    "        label=sequence_labels,\n",
    "        tokens=tokenized_utterances,\n",
    "        token_labels=labels_for_tokens\n",
    "    )\n",
    ")\n",
    "snips_dataset = snips_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "67a6b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'rate homicide: a year on the killing streets five stars',\n",
       " 'label': 1,\n",
       " 'tokens': ['rate',\n",
       "  'homicide:',\n",
       "  'a',\n",
       "  'year',\n",
       "  'on',\n",
       "  'the',\n",
       "  'killing',\n",
       "  'streets',\n",
       "  'five',\n",
       "  'stars'],\n",
       " 'token_labels': [22, 44, 13, 13, 13, 13, 13, 13, 53, 29]}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc13ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to batch tokenize utterances with truncation\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"utterance\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee6381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383065db38084bd1a0cec52c5f3dd274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170c76177e9f46f498d31d1a85d2edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da2a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'play a twenties song',\n",
       " 'label': 0,\n",
       " 'tokens': ['play', 'a', 'twenties', 'song'],\n",
       " 'token_labels': [22, 22, 37, 7],\n",
       " 'input_ids': [101, 1505, 170, 21708, 1461, 102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n",
    "#  length of the longest element in the batch, making them all the same length. \n",
    "#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b14125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator will pad data so that all examples are the same input length.\n",
    "#  Attention mask is how we ignore attention scores for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d647d950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")\n",
    "\n",
    "# set an index -> label dictionary\n",
    "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4db4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlayMusic'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_clf_model.config.id2label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6352abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):  # custom method to take in logits and calculate accuracy of the eval set\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a04da07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # some deep learning parameters that the Trainer is able to take in\n",
    "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
    "    weight_decay = 0.05,\n",
    "    \n",
    "    logging_steps=1,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1c7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 12:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9550352096557617,\n",
       " 'eval_accuracy': 0.09973251815055408,\n",
       " 'eval_runtime': 64.0129,\n",
       " 'eval_samples_per_second': 40.882,\n",
       " 'eval_steps_per_second': 1.281}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f1cf8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running training *****\n",
      "  Num examples = 10467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 22:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.159812</td>\n",
       "      <td>0.971341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.051554</td>\n",
       "      <td>0.987008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-328\n",
      "Configuration saved in ./snips_clf/results/checkpoint-328/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-328/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-656\n",
      "Configuration saved in ./snips_clf/results/checkpoint-656/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_clf/results/checkpoint-656 (score: 0.05155394226312637).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=0.6844381606277283, metrics={'train_runtime': 1376.1424, 'train_samples_per_second': 15.212, 'train_steps_per_second': 0.477, 'total_flos': 131576695874496.0, 'train_loss': 0.6844381606277283, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8fe9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05155394226312637,\n",
       " 'eval_accuracy': 0.9870080244554834,\n",
       " 'eval_runtime': 51.0558,\n",
       " 'eval_samples_per_second': 51.258,\n",
       " 'eval_steps_per_second': 1.606,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e93471e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.9956121444702148}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", sequence_clf_model, tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b373004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./snips_clf/results\n",
      "Configuration saved in ./snips_clf/results/config.json\n",
      "Model weights saved in ./snips_clf/results/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d7d27bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./snips_clf/results/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./snips_clf/results\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PlayMusic\",\n",
      "    \"1\": \"RateBook\",\n",
      "    \"2\": \"SearchScreeningEvent\",\n",
      "    \"3\": \"SearchCreativeWork\",\n",
      "    \"4\": \"BookRestaurant\",\n",
      "    \"5\": \"GetWeather\",\n",
      "    \"6\": \"AddToPlaylist\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./snips_clf/results/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./snips_clf/results\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PlayMusic\",\n",
      "    \"1\": \"RateBook\",\n",
      "    \"2\": \"SearchScreeningEvent\",\n",
      "    \"3\": \"SearchCreativeWork\",\n",
      "    \"4\": \"BookRestaurant\",\n",
      "    \"5\": \"GetWeather\",\n",
      "    \"6\": \"AddToPlaylist\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./snips_clf/results/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./snips_clf/results.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.9956121444702148}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", \"./snips_clf/results\", tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "52fef242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "frozen_sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e8fa35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in frozen_sequence_clf_model.distilbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8dad2bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # some deep learning parameters that the Trainer is able to take in\n",
    "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
    "    weight_decay = 0.05,\n",
    "    \n",
    "    logging_steps=1,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=frozen_sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc1d2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 03:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9511256217956543,\n",
       " 'eval_accuracy': 0.13870844478410393,\n",
       " 'eval_runtime': 33.0914,\n",
       " 'eval_samples_per_second': 79.084,\n",
       " 'eval_steps_per_second': 2.478}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bfa45688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running training *****\n",
      "  Num examples = 10467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 06:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.894000</td>\n",
       "      <td>1.889941</td>\n",
       "      <td>0.356133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.652300</td>\n",
       "      <td>1.679589</td>\n",
       "      <td>0.872755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-328\n",
      "Configuration saved in ./snips_clf/results/checkpoint-328/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-328/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-656\n",
      "Configuration saved in ./snips_clf/results/checkpoint-656/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_clf/results/checkpoint-656 (score: 1.679589033126831).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=1.8726866072634372, metrics={'train_runtime': 377.717, 'train_samples_per_second': 55.422, 'train_steps_per_second': 1.737, 'total_flos': 131576695874496.0, 'train_loss': 1.8726866072634372, 'epoch': 2.0})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # ~23min -> ~6min on my laptop with all of distilbert frozen with a worse loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b911ff29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.679589033126831,\n",
       " 'eval_accuracy': 0.8727550630492931,\n",
       " 'eval_runtime': 32.8797,\n",
       " 'eval_samples_per_second': 79.593,\n",
       " 'eval_steps_per_second': 2.494,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac621132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
