{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97eac2b0",
   "metadata": {},
   "source": [
    "# 1 Introduction to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bdf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5408f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Let's load a vanilla BERT-base model. \n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955f8436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (30522, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "==== First Encoder ====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "named_params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(named_params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in named_params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Encoder ====\\n')\n",
    "for p in named_params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in named_params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b19444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pooler is a separate linear and tanh activated layer that acts on the [CLS] token's representation\n",
    "# This pooled_output is often used as a representation for the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a6505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bert-base uncased tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01cf7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2112, 4048, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Parthi loves a beautiful day')  # tokenize a simple sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0da83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokens through the model\n",
    "\n",
    "#1 Turn tokens_with_unknown_words into a tensor (will be size (8,))\n",
    "#2 Unsqueeze a first dimension to simulate batches. Resulting shape is (1, 8)\n",
    "response = model(torch.tensor(tokenizer.encode('Parthi loves a beautiful day')).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4299db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2491, -0.0460, -0.0651,  ..., -0.5426,  0.2840,  0.3480],\n",
       "         [-0.1203, -0.9860, -0.2574,  ..., -0.0812,  0.4537, -0.3216],\n",
       "         [-0.2920, -0.5337,  0.0832,  ..., -1.4740, -0.2596,  0.5439],\n",
       "         ...,\n",
       "         [-0.0189, -0.0132,  0.4365,  ..., -0.6340,  0.3229, -0.4924],\n",
       "         [-0.3635, -0.5222, -0.0892,  ...,  0.2205,  0.1970, -0.4583],\n",
       "         [ 0.4583, -0.0587, -0.0749,  ..., -0.2100, -0.3157, -0.4889]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.8872e-01, -4.5916e-01, -5.6904e-01,  7.4833e-01,  4.0340e-01,\n",
       "         -5.3119e-02,  9.0267e-01,  3.3290e-01, -2.2647e-01, -9.9999e-01,\n",
       "         -6.0063e-02,  7.8231e-01,  9.9326e-01,  7.9015e-02,  9.5227e-01,\n",
       "         -4.0630e-01,  1.6438e-02, -5.7293e-01,  4.2630e-01, -5.6240e-01,\n",
       "          7.8536e-01,  9.9952e-01,  4.2091e-01,  2.3961e-01,  4.6519e-01,\n",
       "          9.5159e-01, -6.5977e-01,  9.5358e-01,  9.6740e-01,  7.4958e-01,\n",
       "         -5.2202e-01,  2.0167e-01, -9.9414e-01, -1.4530e-01, -7.7832e-01,\n",
       "         -9.9604e-01,  3.6706e-01, -7.5797e-01, -2.3553e-02,  1.3269e-02,\n",
       "         -9.1151e-01,  3.2807e-01,  9.9991e-01, -1.6065e-02,  3.9376e-01,\n",
       "         -2.4546e-01, -1.0000e+00,  2.9702e-01, -9.0306e-01,  4.9580e-01,\n",
       "          4.5537e-01,  3.1933e-01,  6.2672e-02,  5.2381e-01,  4.1462e-01,\n",
       "          1.9785e-01, -2.0306e-02,  1.5762e-01, -2.6538e-01, -5.9816e-01,\n",
       "         -6.3678e-01,  4.4014e-01, -6.4706e-01, -9.3421e-01,  4.6722e-01,\n",
       "          9.7223e-02, -1.0683e-01, -2.8224e-01, -6.6200e-02, -1.4053e-01,\n",
       "          8.7290e-01,  3.0518e-01,  1.9316e-01, -9.0156e-01,  3.9113e-02,\n",
       "          2.8205e-01, -4.0166e-01,  1.0000e+00, -3.2477e-01, -9.8711e-01,\n",
       "          5.6472e-01,  1.7367e-01,  3.2980e-01,  4.7969e-01, -2.3918e-01,\n",
       "         -1.0000e+00,  3.4235e-01, -1.2849e-01, -9.9339e-01,  1.7042e-01,\n",
       "          5.8526e-01, -1.8581e-01,  1.9273e-01,  3.6776e-01, -2.4141e-01,\n",
       "         -4.2686e-01, -3.3041e-01, -5.3879e-01, -3.4831e-01, -2.3717e-01,\n",
       "          5.6296e-02, -2.8095e-01, -1.9714e-01, -3.2709e-01,  2.0207e-01,\n",
       "         -4.2217e-01, -6.6983e-01,  3.3236e-01, -3.3143e-01,  7.2228e-01,\n",
       "          2.1049e-01, -3.3922e-01,  4.1179e-01, -9.6603e-01,  6.7233e-01,\n",
       "         -3.3849e-01, -9.9023e-01, -3.5197e-01, -9.9464e-01,  7.6043e-01,\n",
       "         -2.2483e-01, -3.0288e-01,  9.7376e-01,  8.5549e-03,  2.3770e-01,\n",
       "         -1.2185e-01, -5.3057e-01, -1.0000e+00, -5.2152e-01, -3.7744e-01,\n",
       "          1.6041e-01, -2.7922e-01, -9.8728e-01, -9.5490e-01,  6.4121e-01,\n",
       "          9.5229e-01,  1.6918e-01,  9.9984e-01, -4.1206e-01,  9.6623e-01,\n",
       "          1.6028e-01, -2.5531e-01,  2.8226e-01, -3.9454e-01,  6.0719e-01,\n",
       "          1.7930e-01, -6.4844e-01,  1.7238e-01, -7.9048e-02,  4.2519e-01,\n",
       "         -4.6827e-01, -2.9352e-01, -2.3526e-01, -9.6435e-01, -3.4039e-01,\n",
       "          9.6384e-01, -3.4489e-01, -3.8776e-01,  5.7370e-01, -2.4172e-01,\n",
       "         -5.2835e-01,  8.6059e-01,  3.5561e-01,  4.2407e-01, -1.8658e-01,\n",
       "          4.5621e-01, -2.7702e-02,  5.6687e-01, -8.4630e-01,  1.6655e-01,\n",
       "          3.5803e-01, -2.9983e-01, -4.8429e-01, -9.8894e-01, -3.7824e-01,\n",
       "          5.2034e-01,  9.9198e-01,  8.0269e-01,  2.4755e-01,  5.2195e-01,\n",
       "         -1.5007e-01,  5.1225e-01, -9.6980e-01,  9.8779e-01, -2.6816e-01,\n",
       "          2.6242e-01, -4.1148e-01,  3.1003e-01, -8.4955e-01, -8.5294e-02,\n",
       "          8.4969e-01, -5.6479e-01, -8.3074e-01,  8.0994e-02, -3.7888e-01,\n",
       "         -4.3441e-01, -6.2289e-01,  4.4987e-01, -3.8762e-01, -4.2504e-01,\n",
       "         -9.8381e-02,  9.4734e-01,  9.5218e-01,  8.2964e-01, -3.2882e-01,\n",
       "          6.0327e-01, -9.2346e-01, -3.9528e-01,  1.7504e-01,  1.8641e-01,\n",
       "          2.0078e-01,  9.9656e-01, -3.5655e-01, -8.9576e-02, -9.4685e-01,\n",
       "         -9.9104e-01, -4.8629e-02, -8.9654e-01, -1.1626e-01, -6.5951e-01,\n",
       "          5.3826e-01, -3.4205e-01, -5.5256e-02,  3.9848e-01, -9.8343e-01,\n",
       "         -7.8742e-01,  3.7607e-01, -3.5039e-01,  5.0449e-01, -2.1158e-01,\n",
       "          7.7413e-01,  7.7420e-01, -5.8967e-01,  5.8662e-01,  9.1185e-01,\n",
       "         -7.1469e-01, -8.1412e-01,  7.6478e-01, -2.3484e-01,  8.5109e-01,\n",
       "         -6.8984e-01,  9.9243e-01,  6.1955e-01,  5.2353e-01, -9.6107e-01,\n",
       "         -2.7843e-01, -8.6072e-01, -2.2018e-01, -1.4154e-01, -4.0187e-01,\n",
       "          4.7388e-01,  3.0505e-01,  4.0348e-01,  8.1184e-01, -6.6739e-01,\n",
       "          9.9780e-01, -9.0358e-01, -9.6413e-01, -7.1038e-01, -1.9608e-01,\n",
       "         -9.9219e-01,  7.7562e-01,  3.2957e-01,  2.4139e-01, -4.9807e-01,\n",
       "         -6.3426e-01, -9.7509e-01,  8.7600e-01,  1.1709e-01,  9.8020e-01,\n",
       "         -3.7100e-01, -9.3618e-01, -5.7810e-01, -9.4538e-01, -2.1985e-01,\n",
       "         -1.9302e-01,  3.1781e-01, -8.9297e-02, -9.7636e-01,  5.3055e-01,\n",
       "          5.7515e-01,  5.1134e-01, -1.3364e-01,  9.9855e-01,  1.0000e+00,\n",
       "          9.8318e-01,  9.0779e-01,  8.8450e-01, -9.9826e-01, -7.5858e-01,\n",
       "          9.9999e-01, -9.5579e-01, -1.0000e+00, -9.3203e-01, -6.1019e-01,\n",
       "          2.0087e-01, -1.0000e+00, -1.4145e-01, -4.7280e-02, -9.4927e-01,\n",
       "          1.2788e-01,  9.8629e-01,  9.9231e-01, -1.0000e+00,  9.0940e-01,\n",
       "          9.5420e-01, -4.3423e-01,  8.5738e-01, -4.9141e-01,  9.8249e-01,\n",
       "          3.6371e-01,  5.2968e-01, -2.9411e-01,  4.2566e-01, -7.0797e-01,\n",
       "         -8.6872e-01, -3.9601e-02, -3.8062e-01,  9.8123e-01,  1.7202e-01,\n",
       "         -7.2030e-01, -9.4946e-01,  4.6034e-01, -9.4350e-02, -7.1825e-02,\n",
       "         -9.7458e-01, -2.2172e-01,  3.0543e-01,  7.1815e-01,  1.3219e-01,\n",
       "          3.0017e-01, -7.5605e-01,  1.9667e-01, -5.7184e-01,  3.8923e-01,\n",
       "          5.2078e-01, -9.5309e-01, -5.3657e-01,  2.4631e-01, -3.0191e-01,\n",
       "         -1.4129e-01, -9.5170e-01,  9.7152e-01, -4.9277e-01,  6.1485e-01,\n",
       "          1.0000e+00,  5.6806e-01, -9.0311e-01,  4.7570e-01,  2.3537e-01,\n",
       "         -2.3621e-01,  1.0000e+00,  7.2154e-01, -9.8787e-01, -4.0284e-01,\n",
       "          6.7274e-01, -5.2458e-01, -4.8596e-01,  9.9949e-01, -2.2308e-01,\n",
       "         -5.5982e-02,  6.3916e-02,  9.8513e-01, -9.9445e-01,  9.3182e-01,\n",
       "         -8.9824e-01, -9.8395e-01,  9.7648e-01,  9.5511e-01, -4.7906e-01,\n",
       "         -7.4598e-01,  1.3732e-01, -2.2465e-01,  3.5204e-01, -9.5028e-01,\n",
       "          6.5611e-01,  3.1064e-01, -1.4249e-01,  9.2179e-01, -7.1594e-01,\n",
       "         -3.1182e-01,  3.7509e-01, -7.9754e-02,  1.6585e-01,  7.4487e-01,\n",
       "          6.0623e-01, -2.7008e-01, -7.9884e-04, -2.8657e-01, -5.5932e-01,\n",
       "         -9.7390e-01,  2.1346e-01,  1.0000e+00, -2.2476e-01,  5.0282e-01,\n",
       "         -1.5488e-01, -9.3656e-02, -1.6618e-01,  4.6706e-01,  5.1237e-01,\n",
       "         -3.1246e-01, -8.9162e-01,  5.7057e-01, -9.4187e-01, -9.9271e-01,\n",
       "          6.7253e-01,  1.9972e-01, -2.4679e-01,  9.9998e-01,  3.6440e-01,\n",
       "          3.1618e-01,  1.4166e-01,  8.8348e-01, -5.0883e-02,  3.9804e-01,\n",
       "          2.9418e-01,  9.8424e-01, -2.1961e-01,  3.1594e-01,  8.0170e-01,\n",
       "         -3.8247e-01, -2.6034e-01, -7.0036e-01,  9.2508e-02, -9.4843e-01,\n",
       "          1.5850e-01, -9.7476e-01,  9.6900e-01,  5.5471e-01,  3.9856e-01,\n",
       "          2.0943e-01,  3.8689e-01,  1.0000e+00, -6.7614e-01,  4.9159e-01,\n",
       "         -8.6796e-02,  7.9087e-01, -9.9564e-01, -7.6114e-01, -4.4795e-01,\n",
       "         -3.6436e-02, -1.9544e-01, -2.4451e-01,  3.1323e-01, -9.8170e-01,\n",
       "          3.3842e-02,  4.8259e-01, -9.8408e-01, -9.9571e-01,  2.2590e-03,\n",
       "          7.0404e-01,  1.6794e-01, -9.3577e-01, -7.2451e-01, -6.0422e-01,\n",
       "          5.2969e-01, -2.1378e-01, -9.6536e-01,  4.4635e-01, -3.1065e-01,\n",
       "          4.3530e-01, -1.5436e-01,  3.9358e-01,  6.1141e-02,  8.2779e-01,\n",
       "         -3.2277e-01, -2.5379e-02, -1.2206e-01, -7.4682e-01,  7.1815e-01,\n",
       "         -8.1895e-01, -7.6625e-01, -1.0121e-02,  1.0000e+00, -5.7783e-01,\n",
       "          5.0965e-01,  6.9438e-01,  6.7414e-01, -8.3149e-02,  1.4013e-01,\n",
       "          7.8345e-01,  3.6947e-01, -1.5998e-01, -1.0326e-01, -4.8025e-01,\n",
       "         -4.2342e-01,  4.9160e-01,  2.8084e-01,  6.2474e-02,  8.3736e-01,\n",
       "          7.1736e-01,  1.9088e-01,  1.0371e-01,  4.9981e-02,  9.9946e-01,\n",
       "         -3.0771e-01, -2.4688e-01, -4.1573e-01, -2.9577e-02, -3.7446e-01,\n",
       "          5.2005e-02,  1.0000e+00,  3.4040e-01,  3.6966e-01, -9.9448e-01,\n",
       "         -5.6646e-01, -9.2159e-01,  9.9999e-01,  8.0490e-01, -8.0566e-01,\n",
       "          6.4587e-01,  6.2045e-01, -3.0975e-02,  7.7548e-01, -2.4268e-01,\n",
       "         -1.8681e-01,  3.2023e-01,  1.2613e-01,  9.7351e-01, -4.9385e-01,\n",
       "         -9.7964e-01, -4.8428e-01,  4.1603e-01, -9.7191e-01,  9.9884e-01,\n",
       "         -5.6149e-01, -3.0482e-01, -4.6983e-01, -3.9546e-01,  1.2839e-01,\n",
       "          1.1828e-01, -9.8690e-01, -2.9036e-01,  1.3640e-01,  9.7579e-01,\n",
       "          1.5537e-01, -3.7214e-01, -8.9726e-01,  3.0370e-01,  1.1094e-01,\n",
       "         -5.7699e-01, -9.7415e-01,  9.8540e-01, -9.8308e-01,  6.4877e-01,\n",
       "          1.0000e+00,  1.9881e-01, -3.8695e-01,  1.8491e-01, -3.6294e-01,\n",
       "          3.7805e-01, -5.7151e-01,  5.2108e-01, -9.6782e-01, -3.1008e-01,\n",
       "         -2.2487e-01,  4.3718e-01, -1.2015e-01, -5.7802e-01,  7.9322e-01,\n",
       "          1.3083e-01, -3.0245e-01, -6.0230e-01, -6.2140e-02,  3.8420e-01,\n",
       "          8.1154e-01, -3.1728e-01, -1.2419e-01,  6.2934e-02, -1.2070e-02,\n",
       "         -9.6131e-01, -4.6268e-01, -3.1554e-01, -9.9981e-01,  5.5815e-01,\n",
       "         -1.0000e+00,  2.5276e-01, -3.6640e-01, -1.8177e-01,  8.8003e-01,\n",
       "          6.3616e-01,  4.1169e-01, -8.0087e-01, -1.0146e-01,  8.1961e-01,\n",
       "          8.1775e-01, -2.7215e-01, -1.8963e-01, -7.9383e-01,  3.4497e-01,\n",
       "         -7.0402e-02,  4.6174e-01, -2.3632e-01,  7.2858e-01, -2.0132e-01,\n",
       "          1.0000e+00,  6.9943e-03, -4.8771e-01, -9.7532e-01,  2.4694e-01,\n",
       "         -2.8607e-01,  1.0000e+00, -8.8556e-01, -9.6228e-01,  4.7891e-01,\n",
       "         -5.0153e-01, -8.4480e-01,  3.7076e-01, -7.7403e-02, -7.0756e-01,\n",
       "         -7.4512e-01,  9.5913e-01,  7.3604e-01, -3.7754e-01,  5.5254e-01,\n",
       "         -3.1293e-01, -4.3844e-01, -5.4780e-02,  6.3000e-01,  9.9298e-01,\n",
       "          4.8450e-01,  9.1046e-01, -3.5079e-01, -2.0234e-01,  9.6674e-01,\n",
       "          2.3275e-01,  5.4081e-01,  1.2707e-01,  1.0000e+00,  3.4180e-01,\n",
       "         -9.2280e-01, -1.1276e-02, -9.8520e-01, -1.6120e-01, -9.3147e-01,\n",
       "          3.2678e-01,  1.5562e-01,  9.2981e-01, -2.6362e-01,  9.7589e-01,\n",
       "         -4.5261e-01,  9.5095e-02, -2.9454e-01,  2.4061e-01,  4.9161e-01,\n",
       "         -9.4113e-01, -9.9027e-01, -9.9331e-01,  6.0006e-01, -4.7235e-01,\n",
       "          1.1884e-02,  3.4994e-01,  9.6627e-02,  4.6472e-01,  4.7132e-01,\n",
       "         -1.0000e+00,  9.5765e-01,  4.0417e-01,  4.5468e-01,  9.8048e-01,\n",
       "          5.4458e-01,  5.9078e-01,  2.1841e-01, -9.9050e-01, -9.7899e-01,\n",
       "         -4.2345e-01, -3.1288e-01,  8.0651e-01,  7.4189e-01,  8.6485e-01,\n",
       "          5.0644e-01, -4.8603e-01, -4.4887e-01, -2.7141e-02, -8.7556e-01,\n",
       "         -9.9560e-01,  3.4906e-01,  9.9963e-02, -9.4438e-01,  9.7167e-01,\n",
       "         -4.9299e-01, -6.7398e-02,  4.1472e-01, -5.8666e-01,  9.1293e-01,\n",
       "          8.2081e-01,  4.0402e-01,  2.0126e-01,  5.0796e-01,  9.3773e-01,\n",
       "          9.3341e-01,  9.9336e-01, -5.0200e-01,  7.5021e-01, -2.1764e-01,\n",
       "          5.0818e-01,  7.0668e-01, -9.5964e-01,  1.3022e-01,  2.2515e-01,\n",
       "         -1.8377e-01,  3.4954e-01, -2.3379e-01, -9.4082e-01,  7.0987e-01,\n",
       "         -2.7988e-01,  5.5955e-01, -4.8351e-01,  2.2340e-02, -4.1011e-01,\n",
       "         -1.2043e-01, -7.4413e-01, -5.2901e-01,  4.1788e-01,  9.1762e-02,\n",
       "          9.2368e-01,  8.3840e-01, -4.8393e-02, -6.7315e-01, -2.7393e-01,\n",
       "         -1.3094e-01, -9.3096e-01,  9.0950e-01, -1.2449e-01,  2.1268e-01,\n",
       "          3.4003e-02,  5.5685e-02,  9.0900e-01, -1.5361e-01, -4.2896e-01,\n",
       "         -3.7307e-01, -7.3698e-01,  9.1329e-01, -4.6144e-01, -5.5061e-01,\n",
       "         -5.2097e-01,  8.6435e-01,  3.7538e-01,  9.9981e-01, -3.3314e-01,\n",
       "         -4.9357e-01, -3.9957e-01, -3.5932e-01,  3.2548e-01, -4.1344e-01,\n",
       "         -1.0000e+00,  4.5076e-01, -2.3478e-01,  4.3422e-01, -2.1854e-01,\n",
       "          5.3035e-01, -3.5979e-01, -9.7402e-01, -2.6877e-01,  3.4799e-01,\n",
       "          1.5133e-01, -5.1331e-01, -5.5406e-01,  3.6269e-01, -1.5696e-01,\n",
       "          9.0065e-01,  9.1735e-01,  3.8230e-01,  6.9248e-01,  4.1674e-01,\n",
       "         -1.2905e-01, -7.1747e-01,  9.2724e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b12732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2491, -0.0460, -0.0651,  ..., -0.5426,  0.2840,  0.3480],\n",
       "         [-0.1203, -0.9860, -0.2574,  ..., -0.0812,  0.4537, -0.3216],\n",
       "         [-0.2920, -0.5337,  0.0832,  ..., -1.4740, -0.2596,  0.5439],\n",
       "         ...,\n",
       "         [-0.0189, -0.0132,  0.4365,  ..., -0.6340,  0.3229, -0.4924],\n",
       "         [-0.3635, -0.5222, -0.0892,  ...,  0.2205,  0.1970, -0.4583],\n",
       "         [ 0.4583, -0.0587, -0.0749,  ..., -0.2100, -0.3157, -0.4889]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for each token, the first one being the [CLS] token\n",
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39199fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This layer is trained on top of the Embedding of the CLS token\n",
    "\n",
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c629f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1b30df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the final encoder's representation of the CLS token\n",
    "CLS_embedding = response.last_hidden_state[:, 0, :].unsqueeze(0)\n",
    "\n",
    "CLS_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d061f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b80af031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the embedding for CLS through the pooler gives the same output as the \"pooler_output\"\n",
    "(model.pooler(CLS_embedding) == response.pooler_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45091eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 109,360,128\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f'Total Parameters: {total_params:,}')  # This is where the 110M parameter comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daca46",
   "metadata": {},
   "source": [
    "# 2 Wordpiece tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee92760",
   "metadata": {},
   "source": [
    "## Let's start by taking a look at the Bert Tokenizer.\n",
    "\n",
    "Let's use the `from_pretrained` method to grab the uncased bert-base tokenizer\n",
    "\n",
    "A list of all available modules can be found on their site: https://huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1d8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf5e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of BERT base vocabulary: 30522\n"
     ]
    }
   ],
   "source": [
    "# load the bert-base uncased tokenizer. Quick check what does \"uncased\" mean?\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'Length of BERT base vocabulary: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533e22ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 3722, 6251, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"A simple sentence!\"\n",
    "\n",
    "tokens = tokenizer.encode(text)  # get token ids per BERT-base's vocabulary\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dcd1f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence! [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode will re-construct the sentence with the added [CLS] and [SEP] token\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff94fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "289885d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My friend told me about this class and I love it so far! She was right.. Num tokens: 20\n",
      "Token: 101, subword: [CLS]\n",
      "Token: 2026, subword: my\n",
      "Token: 2767, subword: friend\n",
      "Token: 2409, subword: told\n",
      "Token: 2033, subword: me\n",
      "Token: 2055, subword: about\n",
      "Token: 2023, subword: this\n",
      "Token: 2465, subword: class\n",
      "Token: 1998, subword: and\n",
      "Token: 1045, subword: i\n",
      "Token: 2293, subword: love\n",
      "Token: 2009, subword: it\n",
      "Token: 2061, subword: so\n",
      "Token: 2521, subword: far\n",
      "Token: 999, subword: !\n",
      "Token: 2016, subword: she\n",
      "Token: 2001, subword: was\n",
      "Token: 2157, subword: right\n",
      "Token: 1012, subword: .\n",
      "Token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# A nicer printout  of token ids and token strings\n",
    "\n",
    "print(f'Text: {text}. Num tokens: {len(tokens)}')\n",
    "for t in tokens:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "252eaef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parthi is not in our vocab :'(\n",
    "\n",
    "'parthi' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2710f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 101, subword: [CLS]\n",
      "Token: 2112, subword: part\n",
      "Token: 4048, subword: ##hi\n",
      "Token: 7459, subword: loves\n",
      "Token: 1037, subword: a\n",
      "Token: 3376, subword: beautiful\n",
      "Token: 2154, subword: day\n",
      "Token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unknown_words = 'Parthi loves a beautiful day'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "\n",
    "# We see our sub words in action!\n",
    "for t in tokens_with_unknown_words:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "509ccf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2112, 4048, 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('parthi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "813dfcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 102]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa51549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 101, subword: [CLS]\n",
      "Token: 2112, subword: part\n",
      "Token: 4048, subword: ##hi\n",
      "Token: 2003, subword: is\n",
      "Token: 2256, subword: our\n",
      "Token: 9450, subword: instructor\n",
      "Token: 2005, subword: for\n",
      "Token: 2023, subword: this\n",
      "Token: 12476, subword: awesome\n",
      "Token: 23823, subword: ##sau\n",
      "Token: 3401, subword: ##ce\n",
      "Token: 2465, subword: class\n",
      "Token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unknown_words = 'Parthi is our instructor for this awesomesauce class'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "\n",
    "for t in tokens_with_unknown_words:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb8c9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "# encode_plus gives us token ids, attention mask and segment ids (A vs B). Useful for training time\n",
    "tokens = tokenizer.encode_plus(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c389a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)  # calling the tokenizer directly does the same thing as encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bb04564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python is the 6th token (don't forget the [CLS] token!)\n",
    "python_pet = tokenizer.encode('I love my pet python')\n",
    "\n",
    "# python is the 6th token (don't forget the [CLS] token!)\n",
    "python_language = tokenizer.encode('I love coding in python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57090b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextful embedding of 'python' in 'I love my pet python'\n",
    "python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'python' in 'I love coding in python'\n",
    "python_language_embedding = model(torch.tensor(python_language).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'snake' in 'snake'\n",
    "snake_alone_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:,1,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'programming' in 'programming'\n",
    "programming_alone_embedding = model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:,1,:].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc36a6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_pet_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db56d1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_language_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca9f5ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5843479]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about coding to the word snake\n",
    "cosine_similarity(python_language_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f304c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6928657]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about pets to the word snake. More similar!\n",
    "cosine_similarity(python_pet_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ccae9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49864388]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about pets to the word programming\n",
    "cosine_similarity(python_pet_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "377d87e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5614743]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about coding to the word programming. More similar!\n",
    "cosine_similarity(python_language_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acbcb7",
   "metadata": {},
   "source": [
    "# 3 The many embeddings of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2176ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a0cf97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "679efc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "word_embeddings == context-free word embeddings\n",
    "position_embeddings == encodes word position\n",
    "token_type_embeddings == 0 or 1. Used to lookup the segment embedding\n",
    "\"\"\"\n",
    "\n",
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4c144cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 2112, 4048,  102]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_phrase = 'I am Parthi'\n",
    "\n",
    "# return_tensors='pt' converts to pytorch automatically\n",
    "tokenizer.encode(example_phrase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf13de5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "         [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],\n",
       "         [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],\n",
       "         [ 0.0421, -0.0496,  0.0282,  ..., -0.0365, -0.0707, -0.0363],\n",
       "         [-0.0244, -0.0044, -0.0064,  ..., -0.0226, -0.0268,  0.0279],\n",
       "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context-less embedding of each token in our sentence\n",
    "model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a60688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "         [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],\n",
       "         [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],\n",
       "         [-0.0381, -0.0026,  0.0130,  ...,  0.0038, -0.0279, -0.0082],\n",
       "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the first and last row are the same because they are the \n",
    "#  [CLS] and [SEP] reserved tokens. They are the same without context for every input\n",
    "model.embeddings.word_embeddings(tokenizer.encode('I am Matt', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "675627aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(512, 768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.position_embeddings  # 512 embeddings, one for each position in a max 512 input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82217746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "748eabee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        [-4.1949e-03, -1.1852e-02, -2.1180e-02,  ...,  2.2455e-02,\n",
       "          5.2826e-03, -1.9723e-03],\n",
       "        [-5.6087e-03, -1.0445e-02, -7.2288e-03,  ...,  2.0837e-02,\n",
       "          3.5402e-03,  4.7708e-03],\n",
       "        [-3.0871e-03, -1.8956e-02, -1.8930e-02,  ...,  7.4045e-03,\n",
       "          2.0183e-02,  3.4077e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.position_embeddings(torch.LongTensor(range(6)))  # positional embeddings for our example_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c1e482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 768)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.token_type_embeddings  # 2 embeddings. One for A and one for B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9443c821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([0]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab167901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))  # All tokens have the same embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5208b132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [-3.4025e-04,  5.3974e-01, -2.8805e-01,  ...,  7.5731e-01,\n",
       "           8.9008e-01,  1.6575e-01],\n",
       "         [-6.3496e-01,  1.9748e-01,  2.5116e-01,  ..., -4.0819e-02,\n",
       "           1.3468e+00, -6.9357e-01],\n",
       "         [ 8.8464e-01, -6.0834e-01,  4.2795e-01,  ..., -1.1881e-01,\n",
       "          -8.5422e-01, -5.3264e-01],\n",
       "         [-5.0509e-02,  3.6773e-01,  3.0486e-01,  ...,  2.5110e-01,\n",
       "           4.4811e-02,  9.1143e-01],\n",
       "         [-3.2507e-01, -3.1879e-01, -1.1632e-01,  ..., -3.9602e-01,\n",
       "           4.1120e-01, -7.7552e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feed forward normalization layer\n",
    "\n",
    "model.embeddings.LayerNorm(\n",
    "    model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt')) + \\\n",
    "    model.embeddings.position_embeddings(torch.LongTensor(range(6))) + \\\n",
    "    model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90c64cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [-3.4026e-04,  5.3974e-01, -2.8805e-01,  ...,  7.5731e-01,\n",
       "           8.9008e-01,  1.6575e-01],\n",
       "         [-6.3496e-01,  1.9748e-01,  2.5116e-01,  ..., -4.0819e-02,\n",
       "           1.3468e+00, -6.9357e-01],\n",
       "         [ 8.8464e-01, -6.0834e-01,  4.2795e-01,  ..., -1.1881e-01,\n",
       "          -8.5422e-01, -5.3264e-01],\n",
       "         [-5.0509e-02,  3.6773e-01,  3.0486e-01,  ...,  2.5110e-01,\n",
       "           4.4811e-02,  9.1143e-01],\n",
       "         [-3.2507e-01, -3.1879e-01, -1.1632e-01,  ..., -3.9602e-01,\n",
       "           4.1120e-01, -7.7552e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Et Voilà! The many embeddings of BERT become one embedding per token\n",
    "model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1f696a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec011d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
