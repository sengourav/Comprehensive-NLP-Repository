{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4616e0f",
   "metadata": {},
   "source": [
    "## Pre-trained GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3d6d4",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eee5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from torch import tensor\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', tokenizer=tokenizer)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9982bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The holocaust was a lie. It was a'},\n",
       " {'generated_text': 'The holocaust was a lie. It was a'},\n",
       " {'generated_text': 'The holocaust was an act of genocide. It'},\n",
       " {'generated_text': 'The holocaust was a complete fabrication. It was'},\n",
       " {'generated_text': 'The holocaust was not just a crime against humanity'},\n",
       " {'generated_text': 'The holocaust was a very real event, and'},\n",
       " {'generated_text': 'The holocaust was a genocide. It was a'},\n",
       " {'generated_text': 'The holocaust was a terrible and terrible crime,'},\n",
       " {'generated_text': 'The holocaust was a real thing.\\n\\n'},\n",
       " {'generated_text': 'The holocaust was a major event in the history'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias\n",
    "generator(\"The holocaust was\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "629c5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Jewish people are the only ones who have the right'},\n",
       " {'generated_text': 'Jewish people are not the only ones who are being'},\n",
       " {'generated_text': 'Jewish people are the only ones who are interested in'},\n",
       " {'generated_text': 'Jewish people are not the only ones who have been'},\n",
       " {'generated_text': 'Jewish people are being targeted by ISIS.\\n\\n'},\n",
       " {'generated_text': 'Jewish people are the most marginalized group in the world'},\n",
       " {'generated_text': 'Jewish people are the only ones who are truly oppressed'},\n",
       " {'generated_text': 'Jewish people are not the same as Jews.\\n'},\n",
       " {'generated_text': 'Jewish people are not a problem.\\n\\n\"'},\n",
       " {'generated_text': 'Jewish people are not going to be able to take'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Jewish people are\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe80d246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The earth is a beautiful place, but it is'},\n",
       " {'generated_text': 'The earth is the only place on earth where the'},\n",
       " {'generated_text': 'The earth is not flat. It is not flat'},\n",
       " {'generated_text': 'The earth is flat, and all that is round'},\n",
       " {'generated_text': 'The earth is a beautiful place, and it is'},\n",
       " {'generated_text': 'The earth is flat, and the sky is blue'},\n",
       " {'generated_text': 'The earth is flat.\\n\\n\\nBut you can'},\n",
       " {'generated_text': 'The earth is flat. The sky is flat.'},\n",
       " {'generated_text': 'The earth is a flat sphere with an area of'},\n",
       " {'generated_text': \"The earth is flat, but it's not flat\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The earth is\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbbc554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Demonetization in 2016 was considered to be the most significant change in the nation's political landscape since\"},\n",
       " {'generated_text': 'Demonetization in 2016 was considered a success.\\n\\nThe company is now trying to find a'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered to be one of the most important factors in the success of the'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered a major factor in the rise of the Democratic Party.\\n\\n'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered to be a success.\\n\\nThe results of the study were'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered a \"success\" by many analysts.\\n\\n\"I think'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered a major success.\\n\\n\"We had a lot of people'},\n",
       " {'generated_text': 'Demonetization in 2016 was considered a success, and the organization has since expanded to include more than'},\n",
       " {'generated_text': \"Demonetization in 2016 was considered to be the most successful year in the history of the world's\"},\n",
       " {'generated_text': 'Demonetization in 2016 was considered to be a failure, but it was not.\\n\\nThe'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Demonetization in 2016 was considered\", max_length=20, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "216598e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the winner of 2024 election in India is predicted to be the first Indian to win the Nobel Peace Prize'},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be a woman.\\n\\nThe winner of the'},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be a woman, who is expected to be the'},\n",
       " {'generated_text': \"the winner of 2024 election in India is predicted to be the country's first female president.\\n\\n\"},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be Narendra Modi.\\n\\n\"The winner of'},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be a man named Rahul Gandhi.\\n\\nThe'},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be the first Indian to win the Nobel Peace Prize'},\n",
       " {'generated_text': \"the winner of 2024 election in India is predicted to be the country's first female president.\\n\\n\"},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be a woman.\\n\\nIn a poll conducted'},\n",
       " {'generated_text': 'the winner of 2024 election in India is predicted to be Narendra Modi.\\n\\nThe Indian prime minister'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"the winner of 2024 election in India is predicted to be\", max_length=20, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec88e66",
   "metadata": {},
   "source": [
    "## 7.4 Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60177cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: I hate it when my phone battery dies.\n",
      "Sentiment: Negative\n",
      "###\n",
      "Text: My day has been really great!\n",
      "Sentiment: Positive\n",
      "###\n",
      "Text: Not a fan when it is cloudy\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: Not a fan when it is cloudy\n",
    "Sentiment:\"\"\", top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "582aeb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: I hate it when my phone battery dies.\n",
      "Sentiment: Negative\n",
      "###\n",
      "Text: My day has been really great!\n",
      "Sentiment: Positive\n",
      "###\n",
      "Text: NLP course is going great.\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: NLP course is going great.\n",
    "Sentiment:\"\"\", top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d239b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock.\n",
      "Q: When was Google founded?\n",
      "A: 1998\n",
      "###\n",
      "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julien Chaumond. The company is based in Brooklyn, New York, United States.\n",
      "Q: What does Hugging Face develop?\n",
      "A: social AI-run chatbot applications\n",
      "###\n",
      "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
      "Q: What division do the Jets play in?\n",
      "A: The AFC East\n",
      "###\n",
      "C:\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"\"\"Question/Answering\n",
    "C: Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock.\n",
    "Q: When was Google founded?\n",
    "A: 1998\n",
    "###\n",
    "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julien Chaumond. The company is based in Brooklyn, New York, United States.\n",
    "Q: What does Hugging Face develop?\n",
    "A: social AI-run chatbot applications\n",
    "###\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:\"\"\", top_k=2, num_beams=2, max_length=215, temperature=0.5)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ff20d",
   "metadata": {},
   "source": [
    "## Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11c405c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
      "Q: What division do the Jets play in?\n",
      "A: The Jets play in the AFC East, which is the\n"
     ]
    }
   ],
   "source": [
    "# Same question as before, with no previous examples ie Zero-shot learning. Still works\n",
    "print(generator(\n",
    "    '''Question/Answering\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:''',\n",
    "    top_k=2, num_beams=2, max_length=80, temperature=0.5)[0]['generated_text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4338c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: This new music video was so good\n",
      "Sentiment: I love it\n",
      "Sentiment: I love it\n",
      "Sentiment: I love it\n",
      "Sentiment: I love it\n",
      "Sentiment: I love it\n",
      "Sentiment: I love it\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot doesn't work as much with the sentiment analysis example\n",
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: This new music video was so good\n",
    "Sentiment:\"\"\", top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed778d",
   "metadata": {},
   "source": [
    "# Zero-shot abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2090aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize = \"\"\"This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora.\n",
    "\n",
    "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
    "\n",
    "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f76b05b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Task:\n",
      "This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora.\n",
      "\n",
      "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
      "\n",
      "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
      "TL;DR: This course is a must-have for anyone who has ever been interested in learning and practicing NLPs. It will provide a solid foundation for the NSLP world and will help you to understand how to apply NMLP to real-world problems in real time.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\n",
    "    f\"\"\"Summarization Task:\\n{to_summarize}\\nTL;DR:\"\"\", \n",
    "    max_length=512, top_k=5,  temperature=0.5, no_repeat_ngram_size=2)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfa3f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Task:\n",
      "This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora.\n",
      "\n",
      "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
      "\n",
      "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
      "TL;DR: The NPP-3 model is a fully trained, fully functional, NPS-based model of human behavior. A model that has been trained for a variety of NPL tasks. This trainee model will learn to perform NTP tasks in real-world environments and will use the same model for all the NPs that are currently available. It will also be able to work with the following NPTs: GPP, SPSS, BPT, TPT. In addition, the training model should also include the use of an NPNF model, which can be used as a training environment for other NPI tasks such as NOP, GPN, or TPN. If you would like to learn more about these NIPs, see our NSPT and NPFP training materials.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\n",
    "    f\"\"\"Summarization Task:\\n{to_summarize}\\nTL;DR:\"\"\", \n",
    "    max_length=512, top_k=5, temperature=0.8, no_repeat_ngram_size=2)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485141f7",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
