{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841157cb",
   "metadata": {},
   "source": [
    "# Neural Network Based Embeddings \n",
    "\n",
    "Because relationships in natural language are complex and nonlinear, deep learning models have quickly emerged as an alternative to counting based techniques to generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317db7a0",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec is one of those neural networks based methods that generates embeddings from tokenized, processed text. Various Word2Vec implementations leverage different architecture for the networks, but two common ones are the **Continuous Bag of Words (CBOW)** architecture and the **skip-gram** architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56658d06",
   "metadata": {},
   "source": [
    "![NN](images/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3c5a0",
   "metadata": {},
   "source": [
    "## CBOW vs. Skip Gram\n",
    "\n",
    "\n",
    " CBOW, a feed forward neural network, selects a target and uses distributed representations of the context surrounding the target to predict the target word. The skip-gram architecture is a bit simpler with one hidden layer and strives to predict the probability of a word being present given various inputs. Conceptually, it reverses the input and output of the CBOW approach. The current word is taken as input to the model and it attempts to predict the context around the input word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce21462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = f\"Hey! I'm new in town. Can you please point me in the direction of the groccery store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f553",
   "metadata": {},
   "source": [
    "In order to pre-process our data for the `Word2Vec` model, we'll first need to tokenize the corpus by sentence then by word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by sentence and word \n",
    "data = []\n",
    "\n",
    "for i in sent_tokenize(corpus):\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "        \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d2958",
   "metadata": {},
   "source": [
    "After we've tokenized, we can train a `Word2Vec` model using the small corpus above. Training this model will allow for simliarty calcuations downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec using CBOW\n",
    "cbow = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13030284",
   "metadata": {},
   "source": [
    "The `Word2Vec` class has several parameters.\n",
    "- The `min_count` parameter will ignore any words with less than a single frequency. \n",
    "- The `vector_size` parameter limits the dimensionality of the feature vector\n",
    "- The `window` parameter maps the max distance between the current and predicted word within a sentence \n",
    "- The `sg` parameter controls algorithm. Setting `sg=0` means the CBOW is used, while `sg=1` means that the Skip-Gram algorithm is used \n",
    "\n",
    "Read more about the various parameters [here](https://tedboy.github.io/nlps/_modules/gensim/models/word2vec.html#Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec using Skip Gram\n",
    "skip_gram = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58578c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities \n",
    "cbow_similarity = cbow.wv.similarity(\"town\", \"direction\")\n",
    "skip_gram_similarity = skip_gram.wv.similarity(\"town\", \"direction\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cosine similarity between `town` and `direction` using CBOW Model: {cbow_similarity}\")\n",
    "print(f\"Cosine similarity between `town` and `direction` using Skip Gram Model: {skip_gram_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c6a62",
   "metadata": {},
   "source": [
    "Since our corpus is so small, it's likely that these values will be very similar. As the data grows, the `Word2Vec` model is able to pick up on nuances between words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31392cb1",
   "metadata": {},
   "source": [
    "# Word Embedding using Keras\n",
    " - Define the vocabs size as parameter\n",
    " - convert each word in one hot encoding\n",
    " - Define the feature size as parameter\n",
    " - Convert each word to a vector if sizeof(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98136978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5009270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e59dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=['the glass of milk',\n",
    "      'the glass of juice',\n",
    "      'understanding the meaning of words',\n",
    "      'My name is GOuorav Sen',\n",
    "     'your videos are good',\n",
    "     'who are you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48962639",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93df2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_rep=[one_hot(sentence,voc_size) for sentence in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3fe330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7103, 2242, 808, 349],\n",
       " [7103, 2242, 808, 8023],\n",
       " [1394, 7103, 2223, 808, 4603],\n",
       " [2276, 49, 9182, 1084, 1719],\n",
       " [96, 5729, 8002, 3680],\n",
       " [2505, 8002, 10]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f07b8",
   "metadata": {},
   "source": [
    "## the sentences are of different length so we use pad sequences which adds padding to the vector representation of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209f8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f32e03c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 7103, 2242,  808,  349],\n",
       "       [   0,    0,    0,    0, 7103, 2242,  808, 8023],\n",
       "       [   0,    0,    0, 1394, 7103, 2223,  808, 4603],\n",
       "       [   0,    0,    0, 2276,   49, 9182, 1084, 1719],\n",
       "       [   0,    0,    0,    0,   96, 5729, 8002, 3680],\n",
       "       [   0,    0,    0,    0,    0, 2505, 8002,   10]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_length=8\n",
    "embedded_docs=pad_sequences(indexed_rep,padding='pre',maxlen=max_sent_length)\n",
    "embedded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98612ddf",
   "metadata": {},
   "source": [
    "Clearly each word is represented using its index in vocabulary of size 10k with each vector of length 8 and 0's added in start as pre padding was passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be5080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_length=10\n",
    "model=Sequential(\n",
    "Embedding(voc_size,feature_length,mask_zero=True)\n",
    ")\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dafcbd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [ 0.04734812  0.03247236  0.02644009 -0.03757442  0.02390346\n",
      "    0.02729023  0.03885617 -0.04646749  0.01124547 -0.0367664 ]\n",
      "  [-0.04382465  0.02960933  0.01386192 -0.03950764 -0.0191502\n",
      "   -0.03138149  0.04424322  0.04320553 -0.01413291  0.0482845 ]\n",
      "  [-0.03808812 -0.04608783 -0.02595259  0.03688293  0.04998564\n",
      "   -0.04328847 -0.02208418 -0.02825901 -0.02601417  0.04060811]\n",
      "  [-0.03251044 -0.00560902 -0.0339759   0.03864704  0.04305229\n",
      "    0.02481123 -0.01905105 -0.04114872 -0.00101805  0.00302534]]\n",
      "\n",
      " [[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [ 0.04734812  0.03247236  0.02644009 -0.03757442  0.02390346\n",
      "    0.02729023  0.03885617 -0.04646749  0.01124547 -0.0367664 ]\n",
      "  [-0.04382465  0.02960933  0.01386192 -0.03950764 -0.0191502\n",
      "   -0.03138149  0.04424322  0.04320553 -0.01413291  0.0482845 ]\n",
      "  [-0.03808812 -0.04608783 -0.02595259  0.03688293  0.04998564\n",
      "   -0.04328847 -0.02208418 -0.02825901 -0.02601417  0.04060811]\n",
      "  [ 0.02488638 -0.04389822  0.0172489  -0.03877046  0.00420728\n",
      "   -0.02739899  0.04442513  0.03700123  0.04271991  0.03274259]]\n",
      "\n",
      " [[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.02056844  0.03927263 -0.04681776  0.021491   -0.01057399\n",
      "    0.01909414  0.00578104  0.00938985  0.02353927  0.02380821]\n",
      "  [ 0.04734812  0.03247236  0.02644009 -0.03757442  0.02390346\n",
      "    0.02729023  0.03885617 -0.04646749  0.01124547 -0.0367664 ]\n",
      "  [-0.03400245 -0.03686612 -0.02463676 -0.01084945 -0.03650719\n",
      "   -0.02536966 -0.04218925 -0.02058882 -0.01148617  0.00021671]\n",
      "  [-0.03808812 -0.04608783 -0.02595259  0.03688293  0.04998564\n",
      "   -0.04328847 -0.02208418 -0.02825901 -0.02601417  0.04060811]\n",
      "  [ 0.03969345 -0.00496516  0.02082605 -0.03478463  0.04775092\n",
      "    0.00644487 -0.00986673  0.03756771  0.04657015 -0.01194364]]\n",
      "\n",
      " [[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [ 0.04657208 -0.00486277  0.03536061  0.02452098 -0.02195727\n",
      "   -0.03668691 -0.01308344 -0.02440107  0.04177984 -0.01703253]\n",
      "  [ 0.03429966 -0.01491486 -0.01363365 -0.02327321 -0.00377775\n",
      "    0.02967695  0.02105122  0.01191952  0.0167246  -0.04432927]\n",
      "  [-0.0013616   0.01876101  0.03320359 -0.01525514  0.0313936\n",
      "   -0.04446299 -0.02868102 -0.04024149 -0.00886128  0.00584709]\n",
      "  [-0.04204156  0.01851472  0.01739564 -0.02202437  0.03589154\n",
      "   -0.01701195 -0.01983246 -0.01101335 -0.03493148 -0.01396452]\n",
      "  [ 0.04257015  0.03445014  0.04929464  0.00566089 -0.04993378\n",
      "    0.01097288 -0.01965712  0.03197822  0.01987921  0.01694937]]\n",
      "\n",
      " [[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.03959312 -0.04493466  0.01534637 -0.03108939  0.03969537\n",
      "    0.03488043  0.0086909  -0.03002024  0.04334301 -0.02800143]\n",
      "  [-0.01441591 -0.02722681 -0.0421065  -0.00859029 -0.01707419\n",
      "   -0.04555089  0.02247716 -0.04580845 -0.02892184 -0.03517441]\n",
      "  [ 0.03869328 -0.01536201 -0.04742822  0.01552654  0.00899388\n",
      "    0.02298638 -0.0090248  -0.04802701  0.00807308 -0.00711815]\n",
      "  [-0.01700931  0.00089979  0.02327694 -0.02259687 -0.02828076\n",
      "   -0.00213318  0.00098374 -0.03183194  0.01454712  0.03053209]]\n",
      "\n",
      " [[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443\n",
      "   -0.02005479 -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      "  [ 0.03568515 -0.03359516 -0.04252274 -0.01461344 -0.0418646\n",
      "    0.04539279 -0.01625665  0.03829577 -0.03589972 -0.02457076]\n",
      "  [ 0.03869328 -0.01536201 -0.04742822  0.01552654  0.00899388\n",
      "    0.02298638 -0.0090248  -0.04802701  0.00807308 -0.00711815]\n",
      "  [-0.02689661 -0.02905706  0.0357385   0.03316807 -0.03382139\n",
      "   -0.01127266 -0.02150518 -0.03127136 -0.03253094  0.00434024]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca24e04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 7103, 2242,  808,  349])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05c35c",
   "metadata": {},
   "source": [
    "embedded_docs[0] is a vector of length of 8 and each integer represent a word except 0 which is used as padding. For each word/integer it is converted to a dense vector of feature_length that we took as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4c464b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step\n",
      "[[-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443 -0.02005479\n",
      "  -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      " [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443 -0.02005479\n",
      "  -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      " [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443 -0.02005479\n",
      "  -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      " [-0.01128607 -0.00849074 -0.02349017 -0.04639615  0.04637443 -0.02005479\n",
      "  -0.04243258  0.02892577  0.00104243  0.03111192]\n",
      " [ 0.04734812  0.03247236  0.02644009 -0.03757442  0.02390346  0.02729023\n",
      "   0.03885617 -0.04646749  0.01124547 -0.0367664 ]\n",
      " [-0.04382465  0.02960933  0.01386192 -0.03950764 -0.0191502  -0.03138149\n",
      "   0.04424322  0.04320553 -0.01413291  0.0482845 ]\n",
      " [-0.03808812 -0.04608783 -0.02595259  0.03688293  0.04998564 -0.04328847\n",
      "  -0.02208418 -0.02825901 -0.02601417  0.04060811]\n",
      " [-0.03251044 -0.00560902 -0.0339759   0.03864704  0.04305229  0.02481123\n",
      "  -0.01905105 -0.04114872 -0.00101805  0.00302534]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2f246",
   "metadata": {},
   "source": [
    "# This is how we got embedding of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
