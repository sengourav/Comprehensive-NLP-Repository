{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262aa132",
   "metadata": {},
   "source": [
    "# Embeddings \n",
    "\n",
    "After text has been preprocessed, the next step involves mapping this concise version of the text to numbers, namely vectors. These vector representations of words or phrases are called **embeddings**. There are endless ways to generate these vectors, but we’ll only highlight frequently used techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba30653",
   "metadata": {},
   "source": [
    "## Counting Based Embedding Techniques \n",
    "\n",
    "For the most part, these methods for generating embeddings can be broken down into counting based approaches and more complex, neural network based approaches.Starting with the former, many of the counting based methods have been replaced by their neural network counterparts, but two of the somewhat still popular techniques are Term Frequency Inverse Document Frequency (TF-IDF)  and Bag of Words (BOW). Starting with TF-IDF, this method assigns a score to each word in a document based on its frequency and the frequency of the words in the corpus.Using the product of  term frequency and the inverse document frequency, TF-IDF measures the originality of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51ab5f",
   "metadata": {},
   "source": [
    "It is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529c237",
   "metadata": {},
   "source": [
    "# How to calculate the score:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90db934",
   "metadata": {},
   "source": [
    "![tfidf](images/tfidf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5abb3",
   "metadata": {},
   "source": [
    "## term frequency:\n",
    "The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    "\n",
    "## inverse document frquency:\n",
    "\n",
    "- The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "\n",
    "- So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    " \n",
    "- So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86118c",
   "metadata": {},
   "source": [
    "![tfidf](tfidf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cbd98",
   "metadata": {},
   "source": [
    "# Application:\n",
    "## Information retrieval\n",
    "TF-IDF was invented for document search and can be used to deliver results that are most relevant to what you’re searching for. Imagine you have a search engine and somebody looks for LeBron. The results will be displayed in order of relevance. That’s to say the most relevant sports articles will be ranked higher because TF-IDF gives the word LeBron a higher score.\n",
    "## Keyword Extraction\n",
    "TF-IDF is also useful for extracting keywords from text. How? The highest scoring words of a document are the most relevant to that document, and therefore they can be considered keywords for that document. Pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab563863",
   "metadata": {},
   "source": [
    "In order to generate the *TF-IDF* vector, we'll need to rely on the `sklearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f8ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116b43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Hey! I'm new in town.\"\n",
    "          \"Can you please point me in the direction of the groccery store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380954fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instace of the TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "# Fit the vectorizer to corputs \n",
    "fitted_vectorizer = vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the corpuse using the fit vectorizer \n",
    "X = fitted_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b319f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['can', 'direction', 'groccery', 'hey', 'in', 'me', 'new', 'of',\n",
       "       'please', 'point', 'store', 'the', 'town', 'you'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the feature names \n",
    "fitted_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5a6ab",
   "metadata": {},
   "source": [
    "Note that in the code above, the `fit` and `transform` calls for the vectorizer are broken down into two separate steps. Alternatively, the `fit_transform` method can combine these two steps into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c98df27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             TF-IDF\n",
      "in         0.447214\n",
      "the        0.447214\n",
      "can        0.223607\n",
      "direction  0.223607\n",
      "groccery   0.223607\n",
      "hey        0.223607\n",
      "me         0.223607\n",
      "new        0.223607\n",
      "of         0.223607\n",
      "please     0.223607\n",
      "point      0.223607\n",
      "store      0.223607\n",
      "town       0.223607\n",
      "you        0.223607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gorav sen\\Python\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix into a Pandas DataFrame for later modeling \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X[0].T.todense(), \n",
    "                  index = fitted_vectorizer.get_feature_names(), \n",
    "                  columns=[\"TF-IDF\"]\n",
    "                 )\n",
    "print(df.sort_values(\"TF-IDF\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb8a18",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "Alternatively, BOW describes the occurrence of the words within a document. It counts the frequency of words, ignoring grammar and order, and creates vectors that reflect the importance of words via their frequency in the document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b2571",
   "metadata": {},
   "source": [
    "![bow](images/bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e1c9f",
   "metadata": {},
   "source": [
    "Simliar to *TF-IDF*, we'll leverage `sklearn`'s `CountVectorizer` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858950f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate Count Vectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit to the corpus \n",
    "fitted_vectorizer = vectorizer.fit(corpus)\n",
    "\n",
    "# Transform using fitted vectorizer \n",
    "X = fitted_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e45d669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bag of Words\n",
      "in                    2\n",
      "the                   2\n",
      "can                   1\n",
      "direction             1\n",
      "groccery              1\n",
      "hey                   1\n",
      "me                    1\n",
      "new                   1\n",
      "of                    1\n",
      "please                1\n",
      "point                 1\n",
      "store                 1\n",
      "town                  1\n",
      "you                   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gorav sen\\Python\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix into a Pandas DataFrame for later modeling \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X[0].T.todense(), \n",
    "                  index = fitted_vectorizer.get_feature_names(), \n",
    "                  columns=[\"Bag of Words\"]\n",
    "                 )\n",
    "print(df.sort_values(\"Bag of Words\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b529a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
